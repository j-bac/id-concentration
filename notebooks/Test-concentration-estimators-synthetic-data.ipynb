{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geomle in /home/utilisateur/.local/lib/python3.7/site-packages (1.0)\n",
      "Requirement already satisfied: pandas>=0.19 in /home/utilisateur/.local/lib/python3.7/site-packages (from geomle) (0.25.3)\n",
      "Requirement already satisfied: numpy>=1.13.1 in /home/utilisateur/.local/lib/python3.7/site-packages (from geomle) (1.17.4)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /home/utilisateur/anaconda3/lib/python3.7/site-packages (from geomle) (0.21.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/utilisateur/anaconda3/lib/python3.7/site-packages (from pandas>=0.19->geomle) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/utilisateur/anaconda3/lib/python3.7/site-packages (from pandas>=0.19->geomle) (2.8.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /home/utilisateur/.local/lib/python3.7/site-packages (from scikit-learn>=0.18->geomle) (1.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/utilisateur/.local/lib/python3.7/site-packages (from scikit-learn>=0.18->geomle) (0.14.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/utilisateur/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas>=0.19->geomle) (1.13.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install geomle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from os import path\n",
    "current_folder = path.dirname(path.abspath('')) \n",
    "sys.path.append(current_folder)\n",
    "from estimators import *\n",
    "from geomle import geomle, mle, DataGenerator\n",
    "import multiprocessing as mp\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.linalg import cholesky\n",
    "from scipy.special import gammainc, lambertw\n",
    "import scipy.io\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import umap\n",
    "import seaborn as sns\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import rpy2\n",
    "import rpy2.robjects as ro\n",
    "import rpy2.robjects.numpy2ri\n",
    "import rpy2.robjects.packages as rpackages\n",
    "from functools import wraps\n",
    "import subprocess\n",
    "from IPython.display import display_html\n",
    "from operator import itemgetter\n",
    "ig0 = itemgetter(0)\n",
    "ig1 = itemgetter(1)\n",
    "ig2 = itemgetter(2)\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "utils = rpackages.importr('utils')\n",
    "#utils.install_packages('intrinsicDimension')\n",
    "#utils.install_packages('ider')\n",
    "intdimr = rpackages.importr('intrinsicDimension')\n",
    "ider   = rpackages.importr('ider')\n",
    "r_base = rpackages.importr('base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_side_by_side(*args):\n",
    "    html_str=''\n",
    "    for df in args:\n",
    "        html_str+=df.to_html()\n",
    "    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)\n",
    "\n",
    "def mean_sqe(estimations, truth):\n",
    "    '''\n",
    "    Mean squared error \n",
    "    '''\n",
    "    return ((estimations - truth)^2/truth).sum() /len(truth) \n",
    "    \n",
    "def mean_pe(estimations, truth):\n",
    "    '''\n",
    "    Mean percentage error \n",
    "    '''\n",
    "    return (abs(estimations - truth)/truth).sum() /len(truth)*100\n",
    "\n",
    "def mean_ge(estimations, truth):\n",
    "    '''\n",
    "    Mean geometric error: The geometric mean of the error *ratio*. It is always >= 1.\n",
    "    '''\n",
    "    ratios = np.concatenate(((estimations/truth)[np.newaxis, :], (truth/estimations)[np.newaxis, :]), axis=0)\n",
    "    return np.power(ratios.max(axis=0).prod(), 1.0/len(estimations))\n",
    "\n",
    "def med_pe(estimations, truth):\n",
    "    '''\n",
    "    Median error in %.\n",
    "    '''\n",
    "    return np.percentile(abs(estimations - truth)/truth, q=50)*100\n",
    "\n",
    "\n",
    "def randball(n_points,ndim,radius,center = []):\n",
    "    ''' Generate uniformly sampled ndim-sphere interior'''\n",
    "    if center == []:\n",
    "        center = np.array([0]*ndim)\n",
    "    r = radius\n",
    "    x = np.random.normal(size=(n_points, ndim))\n",
    "    ssq = np.sum(x**2,axis=1)\n",
    "    fr = r*gammainc(ndim/2,ssq/2)**(1/ndim)/np.sqrt(ssq)\n",
    "    frtiled = np.tile(fr.reshape(n_points,1),(1,ndim))\n",
    "    p = center + np.multiply(x,frtiled)\n",
    "    return p, center\n",
    "\n",
    "def proxy(tup):\n",
    "    function,X,Dict = tup\n",
    "    return function(X,**Dict)\n",
    "\n",
    "def get_nn(X,k,n_jobs=1):\n",
    "    neigh = NearestNeighbors(n_neighbors=k,n_jobs=n_jobs)\n",
    "    neigh.fit(X)\n",
    "    dists, inds = neigh.kneighbors(return_distance=True)\n",
    "    return dists,inds\n",
    "\n",
    "def asPointwise(data,function, params, precomputed_knn = None, n_neighbors=100, n_jobs=1):\n",
    "    '''Use a global estimator as a pointwise one by creating kNN neighborhoods'''\n",
    "    if precomputed_knn is not None:\n",
    "        knn = precomputed_knn\n",
    "    else:\n",
    "        _, knn = get_nn(data, k=n_neighbors, n_jobs=n_jobs)\n",
    "        \n",
    "    if n_jobs > 1:\n",
    "        pool = mp.Pool(n_jobs)\n",
    "        results = pool.map(proxy,[(function,data[i,:],params) for i in knn])\n",
    "        pool.close()\n",
    "        return results\n",
    "    else:\n",
    "        return [function(data[i,:],**params) for i in knn]\n",
    "\n",
    "\n",
    "from functools import wraps\n",
    "def calculate_time(func): \n",
    "    @wraps(func)\n",
    "    def inner_func(*args, **kwargs): \n",
    "        begin = time.time() \n",
    "        res = func(*args, **kwargs) \n",
    "        end = time.time()\n",
    "        return res, end - begin\n",
    "    return inner_func\n",
    "\n",
    "class DimEst():\n",
    "    def __init__(self):\n",
    "        self.names = ['MLE', 'GeoMLE', 'MIND', 'DANCo', 'FastDANCo', 'ESS', 'PCA', 'CD','FisherS','ANOVA','TwoNN']\n",
    "        self.caldatas = {}\n",
    "        \n",
    "    def estimateAllMethods(self, data,ConditionalNumber=10):\n",
    "        dim = data.shape[1]\n",
    "        self.funcs = {'MLE':          self.mle(data),\n",
    "                      #'GeoMLE':       self.geomle(data, dim),\n",
    "                      #'DANCo':        self.danco(data, dim),\n",
    "                      'FastDANCo':    self.fast_danco(data),\n",
    "                      #'ESS':          self.ess(data),\n",
    "                      'PCA':          self.pca(data),\n",
    "                      #'CD':           self.cd(data),\n",
    "                      'FisherS':      self.fisherS(data,ConditionalNumber),\n",
    "                      'ANOVA':        self.anova(data),\n",
    "                      'TwoNN':        self.twonn(data)\n",
    "                     }\n",
    "                      \n",
    "        self.times = {key: ig1(val) for key, val in self.funcs.items()}\n",
    "        self.funcs = {key: ig0(val) for key, val in self.funcs.items()}\n",
    "        return self.funcs, self.times\n",
    "    \n",
    "    def estimateAllMethodsLocally(self, data, k, n_jobs = 1, ConditionalNumber = 10):\n",
    "        dim = data.shape[1]\n",
    "        \n",
    "        _, knn = get_nn(data, k, n_jobs)\n",
    "        \n",
    "        mle_pw, tle_pw, mom_pw, ed_pw, ged_pw, pca_pw = self.rado_ests(data,k).values()\n",
    "        self.funcs = {'MLE':          self.mlelocal(data,k),\n",
    "                      #'GeoMLE':       self.geomlelocal(data, dim),\n",
    "                      'mind_mlk':         asPointwise(data,self.mind_mlk,{'dim':dim},precomputed_knn=knn,n_jobs=1),\n",
    "                      'mind_mli':         asPointwise(data,self.mind_mli,{'dim':dim},precomputed_knn=knn,n_jobs=1),\n",
    "                      #'DANCo':        asPointwise(data,self.danco,{'dim':dim},precomputed_knn=knn,n_jobs=1),\n",
    "                      'FastDANCo':    self.fast_dancoloop(data),\n",
    "                      'ESS':          asPointwise(data,self.ess,{},precomputed_knn=knn,n_jobs=1),\n",
    "                      #'PCA':          self.pca(data),\n",
    "                      'CD':           asPointwise(data,self.lcd,{},precomputed_knn=knn,n_jobs=1),\n",
    "                      'FisherS':      asPointwise(data,self.fisherS,{'ConditionalNumber':ConditionalNumber},precomputed_knn=knn,n_jobs=n_jobs),\n",
    "                      'ANOVA':        self.anovalocal(data,k),\n",
    "                      'TwoNN':        asPointwise(data,self.twonn,{},precomputed_knn=knn,n_jobs=n_jobs),\n",
    "                      'radoMLE':      mle_pw,\n",
    "                      'radoTLE':      tle_pw,\n",
    "                      'radoMOM':      mom_pw,\n",
    "                      'radoED':       ed_pw,\n",
    "                      'radoGED':      ged_pw,\n",
    "                      'radoPCA':      pca_pw\n",
    "                     }\n",
    "                      \n",
    "        self.times = {}\n",
    "        for key, val in self.funcs.items():\n",
    "            if key in ['MLE','ANOVA','FastDANCo']:\n",
    "                self.funcs[key] = np.array(val[0])\n",
    "                self.times[key] = val[1]\n",
    "            elif 'rado' in key:\n",
    "                self.funcs[key] = np.array(val)\n",
    "            else:\n",
    "                self.funcs[key] = np.array([i[0] for i in val])\n",
    "                self.times[key] = np.sum([i[1] for i in val])\n",
    "            \n",
    "        return self.funcs, self.times\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def rado_ests(data,k):\n",
    "        return radovanovic_estimators_matlab(data,k=k)\n",
    "        mle_pw, tle_pw, mom_pw, ed_pw, ged_pw, pca_pw = rado_ests.values()\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def mle(data):\n",
    "        return intdimr.maxLikGlobalDimEst(data,k=20).rx2('dim.est')[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def mlelocal(data,k):\n",
    "        res = intdimr.maxLikPointwiseDimEst(data,k=k)\n",
    "        return np.array([i[0] for i in res])\n",
    "\n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def geomle(data, dim):\n",
    "#         k1 =  k1_log(dim)\n",
    "#         k2 =  k2_log(dim)\n",
    "        return geomle(pd.DataFrame(data), k1=20, k2=55, nb_iter1=1, alpha=5e-3).mean()\n",
    "\n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def geomlelocal(data, dim):\n",
    "#         k1 =  k1_log(dim)\n",
    "#         k2 =  k2_log(dim)\n",
    "        return geomle(pd.DataFrame(data), k1=20, k2=55, nb_iter1=1, alpha=5e-3)\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def mind_mlk(data, dim):\n",
    "        return intdimr.dancoDimEst(data, k=10, D=min(dim,100), ver=\"MIND_MLk\").rx2('dim.est')[0]\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def mind_mli(data, dim):\n",
    "        return intdimr.dancoDimEst(data, k=10, D=min(dim,100), ver=\"MIND_MLi\").rx2('dim.est')[0]\n",
    "    \n",
    "    #@staticmethod\n",
    "    @calculate_time\n",
    "    def danco(self,data, dim):\n",
    "        try:\n",
    "            res = intdimr.dancoDimEst(data, k=10, D=min(dim,100), calibration_data = self.caldatas[len(data)], ver=\"DANCo\")\n",
    "            self.caldatas[len(data)]=res[2]\n",
    "            return res.rx2('dim.est')[0]\n",
    "        except:\n",
    "            res = intdimr.dancoDimEst(data, k=10, D=min(dim,100), ver=\"DANCo\")\n",
    "            self.caldatas[len(data)]=res[2]\n",
    "            return res.rx2('dim.est')[0]\n",
    "\n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def fast_danco(data):\n",
    "        return runDANCo(data)[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def fast_dancoloop(data):\n",
    "        return runDANColoop(data)\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def ess(data):\n",
    "        return ess_py(data)[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def pca(data):\n",
    "        return intdimr.pcaLocalDimEst(data, 'FO').rx2('dim.est')[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def cd(data):\n",
    "        return corint_py(data, k1=10, k2=20)[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def lcd(data):\n",
    "        return corint_py(data, k1=10, k2=len(data)-1)[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def fisherS(data,ConditionalNumber):\n",
    "        return SeparabilityAnalysis(data,ProducePlots=0,alphas=np.arange(.2,1,.02)[None],ConditionalNumber=ConditionalNumber)[1][0]\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def anova(data):\n",
    "        return runANOVAglobal(data)[0,0]\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def anovalocal(data,k):\n",
    "        return runANOVAlocal(data,k=k)[:,0]\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def twonn(data):\n",
    "        res = twonn_py(data)\n",
    "        return res    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DE=DimEst()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = list(filter(lambda x: '.data' in x, os.listdir('../data/id-tle-synth-m10000-data/data/m10000/')))\n",
    "synthetic_data = [np.array(pd.read_csv('../data/id-tle-synth-m10000-data/data/m10000/'+file,sep=' ',header=None)) for file in data_files]\n",
    "synthetic_data = dict(zip(data_files,synthetic_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global ID saturation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " m10a-09.data\n",
      "Running subsampling analysis...\n",
      "Subsample percentages = [1, 2, 4, 8, 16, 32, 64]\n",
      "Number of repeats = 1\n",
      "Number of samples = 10000\n",
      "Dimension = 11\n",
      "----------------------------\n",
      "\n",
      "Sample size = 50\n",
      "Elapsed time = 6.547706842422485\n",
      "Sample size = 100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-135e756ea3f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m#Run estimators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mallres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimateAllMethods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mallres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mruntimes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-9bb71083cb2c>\u001b[0m in \u001b[0;36mestimateAllMethods\u001b[0;34m(self, data, ConditionalNumber)\u001b[0m\n\u001b[1;32m     89\u001b[0m                       \u001b[0;31m#'GeoMLE':       self.geomle(data, dim),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                       \u001b[0;31m#'DANCo':        self.danco(data, dim),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                       \u001b[0;34m'FastDANCo'\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_danco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m                       \u001b[0;31m#'ESS':          self.ess(data),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                       \u001b[0;34m'PCA'\u001b[0m\u001b[0;34m:\u001b[0m          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-9bb71083cb2c>\u001b[0m in \u001b[0;36minner_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mbegin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbegin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-9bb71083cb2c>\u001b[0m in \u001b[0;36mfast_danco\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcalculate_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfast_danco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrunDANCo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/id-concentration/estimators/_call_estimators.py\u001b[0m in \u001b[0;36mrunDANCo\u001b[0;34m(data, k, path_to_estimators, path_to_matlab)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_estimators\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'temp_data.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     subprocess.call([path_to_matlab,\"-nodisplay\", \"-nosplash\", \"-nodesktop\",\"-nojvm\",\"-r\",\n\u001b[0;32m---> 44\u001b[0;31m                         \"data=dlmread('\"+path_to_estimators+\"/temp_data.txt');cd ('\"+path_to_estimators+\"');k=\"+str(k)+\";[d,kl,mu2,tau2] = DANCoFit(data',k);save('variables.mat');exit;\"\"\"])\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_estimators\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'variables.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'd'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Including KeyboardInterrupt, wait handled that.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    988\u001b[0m             \u001b[0mendtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m             \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1622\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m                             \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Another thread waited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1624\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1625\u001b[0m                         \u001b[0;31m# Check the pid and loop as waitpid has been known to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1626\u001b[0m                         \u001b[0;31m# return 0 even without WNOHANG in odd situations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1580\u001b[0m             \u001b[0;34m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mChildProcessError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m                 \u001b[0;31m# This happens if SIGCLD is set to be ignored or waiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# testing separability saturation\n",
    "\n",
    "for key,data in synthetic_data.items():\n",
    "    n_repeats = 1\n",
    "    sample_sizes = [1,2,4,8,16,32,64]\n",
    "    sample_sizes_halves = [x/2 for x in sample_sizes]\n",
    "\n",
    "    all_sample_sizes = sample_sizes+sample_sizes_halves\n",
    "    all_sample_sizes = list(set(all_sample_sizes))\n",
    "    all_sample_sizes.sort()\n",
    "    \n",
    "    \n",
    "    datasets_done = [i.split('_')[0] for i in list(filter(lambda x: '.data' in x, os.listdir('../results')))]\n",
    "    dataset_name = key\n",
    "    n_samples = data.shape[0]\n",
    "    \n",
    "    if dataset_name in datasets_done:\n",
    "        print('already computed ', dataset_name)\n",
    "        continue\n",
    "    \n",
    "    print('\\n',dataset_name)\n",
    "    print('Running subsampling analysis...\\nSubsample percentages = {}\\nNumber of repeats = {}\\nNumber of samples = {}\\nDimension = {}'.format(sample_sizes,n_repeats,n_samples,data.shape[1]))\n",
    "    print('----------------------------\\n')\n",
    "    \n",
    "    n_methods = 6\n",
    "    all_dim_estimates = np.empty([n_methods,len(all_sample_sizes)+1,n_repeats])\n",
    "\n",
    "    runtimes = []\n",
    "    for i,sz in enumerate(all_sample_sizes):\n",
    "        sample_size = int(n_samples*sz/100)\n",
    "        print('Sample size = {}'.format(sample_size))\n",
    "        start_time = time.time()\n",
    "        for j in range(0,n_repeats):\n",
    "            sample = np.random.choice(n_samples,replace=False, size=sample_size)\n",
    "            xs = data[sample,:]\n",
    "\n",
    "            #Run estimators\n",
    "            allres = DE.estimateAllMethods(xs)\n",
    "            results = allres[0]\n",
    "            runtimes.append(allres[1])\n",
    "\n",
    "            #Store\n",
    "            all_dim_estimates[0,i,j] = results['FisherS']\n",
    "            all_dim_estimates[1,i,j] = results['FastDANCo']\n",
    "            all_dim_estimates[2,i,j] = results['TwoNN']\n",
    "            all_dim_estimates[3,i,j] = results['ANOVA']\n",
    "            all_dim_estimates[4,j] = results['MLE']\n",
    "            all_dim_estimates[5,j] = results['PCA']\n",
    "\n",
    "\n",
    "        print(\"Elapsed time = {}\".format(time.time()-start_time))\n",
    "\n",
    "    allres = DE.estimateAllMethods(data)\n",
    "    results = allres[0]\n",
    "    runtimes.append(allres[1])\n",
    "\n",
    "    for i in range(0,n_repeats):\n",
    "        all_dim_estimates[0,len(all_sample_sizes),i] = results['FisherS']\n",
    "        all_dim_estimates[1,len(all_sample_sizes),i] = results['FastDANCo']\n",
    "        all_dim_estimates[2,len(all_sample_sizes),i] = results['TwoNN']\n",
    "        all_dim_estimates[3,len(all_sample_sizes),i] = results['ANOVA']\n",
    "        all_dim_estimates[4,len(all_sample_sizes),i] = results['MLE']\n",
    "        all_dim_estimates[5,len(all_sample_sizes),i] = results['PCA']\n",
    "\n",
    "\n",
    "    all_sample_sizes.append(100)\n",
    "    sample_sizes.append(100)\n",
    "\n",
    "    np.savetxt(\"../results/\"+dataset_name+\"_all_dim_estimates.txt\", all_dim_estimates, delimiter=\"\\t\")\n",
    "    np.savetxt(\"../results/\"+dataset_name+\"_all_sample_sizes.txt\", all_sample_sizes, delimiter=\"\\t\")\n",
    "    np.savetxt(\"../results/\"+dataset_name+\"_sample_sizes.txt\", sample_sizes, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot convergence curve\n",
    "alls=pd.read_csv('../results/'+dataset_name+'_all_sample_sizes.txt', sep='\\t',header=None)\n",
    "all_sample_sizes = alls.to_numpy()[:,0]\n",
    "sizes=pd.read_csv('../results/'+dataset_name+'_sample_sizes.txt', sep='\\t',header=None)\n",
    "sample_sizes =sizes.to_numpy()[:,0]\n",
    "\n",
    "estimators = ['fisherS_dim_estimates','ess_dim_estimates','danco_dim_estimates',\n",
    "              'twonn_dim_estimates','anova_dim_estimates','mle_dim_estimates','mind_dim_estimates']\n",
    "\n",
    "for estimator in estimators:\n",
    "    print(estimator)\n",
    "    ds=pd.read_csv('../results/'+dataset_name+'_'+estimator+'.txt', sep='\\t',header=None)\n",
    "    dim_estimates=ds.to_numpy()\n",
    "\n",
    "    mn = np.mean(dim_estimates[:,:],1)\n",
    "    std = np.std(dim_estimates[:,:],1)\n",
    "\n",
    "    plt.figure(figsize=(10,3))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(all_sample_sizes,mn,'bs-')\n",
    "    plt.plot(all_sample_sizes,mn-std,'r--')\n",
    "    plt.plot(all_sample_sizes,mn+std,'r--')\n",
    "    plt.plot(all_sample_sizes,dim_estimates,'b+')\n",
    "    plt.xlabel('Percentage of points')\n",
    "    plt.ylabel('Estimated intrinsic dimension')\n",
    "\n",
    "    ratios = []\n",
    "    for sz in sample_sizes:\n",
    "        sz_half = sz/2\n",
    "        k = [i for i,asz in enumerate(all_sample_sizes) if np.abs(sz-asz)<0.001 ][0]\n",
    "        k_half = [i for i,asz in enumerate(all_sample_sizes) if np.abs(sz_half-asz)<0.001 ][0]\n",
    "        #print(k,all_sample_sizes[k],k_half,all_sample_sizes[k_half])\n",
    "        ratios.append(1-std[k]/std[k_half])\n",
    "        \n",
    "    #avoid case of 0 std (nan ratio)\n",
    "    ratios=np.array(ratios)\n",
    "    ratios[np.isnan(ratios)]=1\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.plot(sample_sizes,ratios,'bs-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local estimates convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "poker=np.array(pd.read_csv('../data/poker-hand-training-true.data',header=None))\n",
    "n_samples = poker.shape[0]\n",
    "np.random.seed(0);subsample = np.random.choice(n_samples,replace=False, size=5000)\n",
    "poker = poker[subsample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,data in [('poker',poker)]:\n",
    "    n_repeats = 1\n",
    "    sample_sizes = [5,7,10,15,20,25,30,50,70,90]\n",
    "    sample_sizes_halves = [x/2 for x in sample_sizes]\n",
    "\n",
    "    all_sample_sizes = sample_sizes+sample_sizes_halves\n",
    "    all_sample_sizes = list(set(all_sample_sizes))\n",
    "    all_sample_sizes.sort()\n",
    "    \n",
    "    \n",
    "    datasets_done = [i.split('_')[0] for i in list(filter(lambda x: '.data' in x, os.listdir('../results')))]\n",
    "    dataset_name = key\n",
    "    n_samples = data.shape[0]\n",
    "    \n",
    "    if dataset_name in datasets_done:\n",
    "        print('already computed ', dataset_name)\n",
    "        continue\n",
    "    \n",
    "    print('\\n',dataset_name)\n",
    "    print('Running subsampling analysis...\\nSubsample percentages = {}\\nNumber of repeats = {}\\nNumber of samples = {}\\nDimension = {}'.format(sample_sizes,n_repeats,n_samples,data.shape[1]))\n",
    "    print('----------------------------\\n')\n",
    "    \n",
    "    for i,sz in enumerate(all_sample_sizes):\n",
    "        sample_size = int(n_samples*sz/100)\n",
    "        print('Sample size = {}'.format(sample_size))\n",
    "        start_time = time.time()\n",
    "        for j in range(0,n_repeats):\n",
    "            sample = np.random.choice(n_samples,replace=False, size=sample_size)\n",
    "            xs = data[sample,:]\n",
    "\n",
    "    \n",
    "    ests_pw_dict_all_neighbors = []\n",
    "    num_neighbors = [25,50,100,200]\n",
    "    n_jobs = 4\n",
    "    #Run ID estimators pointwise in KNN neighborhoods of different sizes\n",
    "    for n_neighbors in num_neighbors:\n",
    "        for dataset_name,data in [('mnist',real_data_subsampled)]:\n",
    "\n",
    "            n_samples = data.shape[0]\n",
    "            print(dataset_name)\n",
    "            print('Running kNN ID for all estimators...\\nNumber of samples = {}\\nDimension = {}'.format(n_samples,data.shape[1]))\n",
    "            print('----------------------------\\n')\n",
    "            print('kNN = ',n_neighbors)\n",
    "\n",
    "            start_all=time.time()\n",
    "\n",
    "            ests_pw_dict = DE.estimateAllMethodsLocally(data, k = n_neighbors, n_jobs = n_jobs, ConditionalNumber = np.inf)\n",
    "\n",
    "            print('elapsed :',round(time.time()-start_all,2))\n",
    "\n",
    "            with open('../results/ests_pw_dict_'+dataset_name+'_kNN'+str(n_neighbors)+'.pkl','wb') as f:\n",
    "                pickle.dump(ests_pw_dict,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study kNN ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist\n",
      "Running kNN ID for all estimators...\n",
      "Number of samples = 2000\n",
      "Dimension = 784\n",
      "----------------------------\n",
      "\n",
      "kNN =  100\n"
     ]
    }
   ],
   "source": [
    "num_neighbors = [100]\n",
    "n_jobs = 4\n",
    "\n",
    "#Run ID estimators pointwise in KNN neighborhoods of different sizes\n",
    "for n_neighbors in num_neighbors:\n",
    "    for dataset_name,data in [('mnist',real_data_subsampled)]:\n",
    "    \n",
    "        n_samples = data.shape[0]\n",
    "        print(dataset_name)\n",
    "        print('Running kNN ID for all estimators...\\nNumber of samples = {}\\nDimension = {}'.format(n_samples,data.shape[1]))\n",
    "        print('----------------------------\\n')\n",
    "        print('kNN = ',n_neighbors)\n",
    "\n",
    "        start_all=time.time()\n",
    "\n",
    "        ests_pw_dict = DE.estimateAllMethodsLocally(data, k = n_neighbors, n_jobs = n_jobs)\n",
    "\n",
    "        print('elapsed :',round(time.time()-start_all,2))\n",
    "\n",
    "        with open('../results/ests_pw_dict_'+dataset_name+'_kNN'+str(n_neighbors)+'.pkl','wb') as f:\n",
    "            pickle.dump(ests_pw_dict,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e711e868ef19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnum_neighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m425\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "num_neighbors = np.arange(25,425,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../results/ests_pw_dict_'+dataset_name+'_kNN'+str(n_neighbors)+'.pkl','rb') as f:\n",
    "    res_pw=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study global pointwise ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_global_id = []\n",
    "list_inseparability_id = []\n",
    "for dataset_name,data in list(real_data_subsampled.items())[:1]:\n",
    "\n",
    "    print(dataset_name)\n",
    "    start_all=time.time()\n",
    "\n",
    "    [n_alpha,n_single,p_alpha,alphas,separable_fraction,Xp] = SeparabilityAnalysis(data,ProducePlots=0)\n",
    "    n_pointwise, idx = point_inseparability_to_pointID(n_alpha,n_single,p_alpha,alphas,idx='all_separable')\n",
    "    \n",
    "    list_global_id.append(n_single[0])\n",
    "    list_inseparability_id.append(n_pointwise)\n",
    "\n",
    "    print('elapsed :',round(time.time()-start_all,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study the behavior of the statistics used by the various estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
