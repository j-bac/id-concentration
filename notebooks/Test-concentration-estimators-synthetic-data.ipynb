{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geomle in /home/utilisateur/.local/lib/python3.7/site-packages (1.0)\n",
      "Requirement already satisfied: pandas>=0.19 in /home/utilisateur/.local/lib/python3.7/site-packages (from geomle) (0.25.3)\n",
      "Requirement already satisfied: numpy>=1.13.1 in /home/utilisateur/.local/lib/python3.7/site-packages (from geomle) (1.17.4)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /home/utilisateur/anaconda3/lib/python3.7/site-packages (from geomle) (0.21.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/utilisateur/anaconda3/lib/python3.7/site-packages (from pandas>=0.19->geomle) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/utilisateur/anaconda3/lib/python3.7/site-packages (from pandas>=0.19->geomle) (2.8.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /home/utilisateur/.local/lib/python3.7/site-packages (from scikit-learn>=0.18->geomle) (1.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/utilisateur/.local/lib/python3.7/site-packages (from scikit-learn>=0.18->geomle) (0.14.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/utilisateur/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas>=0.19->geomle) (1.13.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install geomle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from os import path\n",
    "current_folder = path.dirname(path.abspath('')) \n",
    "sys.path.append(current_folder)\n",
    "from estimators import *\n",
    "from geomle import geomle, mle, DataGenerator\n",
    "import multiprocessing as mp\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.linalg import cholesky\n",
    "from scipy.special import gammainc, lambertw\n",
    "import scipy.io\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import umap\n",
    "import seaborn as sns\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import rpy2\n",
    "import rpy2.robjects as ro\n",
    "import rpy2.robjects.numpy2ri\n",
    "import rpy2.robjects.packages as rpackages\n",
    "from functools import wraps\n",
    "import subprocess\n",
    "from IPython.display import display_html\n",
    "from operator import itemgetter\n",
    "ig0 = itemgetter(0)\n",
    "ig1 = itemgetter(1)\n",
    "ig2 = itemgetter(2)\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "utils = rpackages.importr('utils')\n",
    "#utils.install_packages('intrinsicDimension')\n",
    "#utils.install_packages('ider')\n",
    "intdimr = rpackages.importr('intrinsicDimension')\n",
    "ider   = rpackages.importr('ider')\n",
    "r_base = rpackages.importr('base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_side_by_side(*args):\n",
    "    html_str=''\n",
    "    for df in args:\n",
    "        html_str+=df.to_html()\n",
    "    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)\n",
    "\n",
    "def mean_sqe(estimations, truth):\n",
    "    '''\n",
    "    Mean squared error \n",
    "    '''\n",
    "    return ((estimations - truth)^2/truth).sum() /len(truth) \n",
    "    \n",
    "def mean_pe(estimations, truth):\n",
    "    '''\n",
    "    Mean percentage error \n",
    "    '''\n",
    "    return (abs(estimations - truth)/truth).sum() /len(truth)*100\n",
    "\n",
    "def mean_ge(estimations, truth):\n",
    "    '''\n",
    "    Mean geometric error: The geometric mean of the error *ratio*. It is always >= 1.\n",
    "    '''\n",
    "    ratios = np.concatenate(((estimations/truth)[np.newaxis, :], (truth/estimations)[np.newaxis, :]), axis=0)\n",
    "    return np.power(ratios.max(axis=0).prod(), 1.0/len(estimations))\n",
    "\n",
    "def med_pe(estimations, truth):\n",
    "    '''\n",
    "    Median error in %.\n",
    "    '''\n",
    "    return np.percentile(abs(estimations - truth)/truth, q=50)*100\n",
    "\n",
    "\n",
    "def randball(n_points,ndim,radius,center = []):\n",
    "    ''' Generate uniformly sampled ndim-sphere interior'''\n",
    "    if center == []:\n",
    "        center = np.array([0]*ndim)\n",
    "    r = radius\n",
    "    x = np.random.normal(size=(n_points, ndim))\n",
    "    ssq = np.sum(x**2,axis=1)\n",
    "    fr = r*gammainc(ndim/2,ssq/2)**(1/ndim)/np.sqrt(ssq)\n",
    "    frtiled = np.tile(fr.reshape(n_points,1),(1,ndim))\n",
    "    p = center + np.multiply(x,frtiled)\n",
    "    return p, center\n",
    "\n",
    "def proxy(tup):\n",
    "    function,X,Dict = tup\n",
    "    return function(X,**Dict)\n",
    "\n",
    "def get_nn(X,k,n_jobs=1):\n",
    "    neigh = NearestNeighbors(n_neighbors=k,n_jobs=n_jobs)\n",
    "    neigh.fit(X)\n",
    "    dists, inds = neigh.kneighbors(return_distance=True)\n",
    "    return dists,inds\n",
    "\n",
    "def asPointwise(data,function, params, precomputed_knn = None, n_neighbors=100, n_jobs=1):\n",
    "    '''Use a global estimator as a pointwise one by creating kNN neighborhoods'''\n",
    "    if precomputed_knn is not None:\n",
    "        knn = precomputed_knn\n",
    "    else:\n",
    "        _, knn = get_nn(data, k=n_neighbors, n_jobs=n_jobs)\n",
    "        \n",
    "    if n_jobs > 1:\n",
    "        pool = mp.Pool(n_jobs)\n",
    "        results = pool.map(proxy,[(function,data[i,:],params) for i in knn])\n",
    "        pool.close()\n",
    "        return results\n",
    "    else:\n",
    "        return [function(data[i,:],**params) for i in knn]\n",
    "\n",
    "def dimension_uniform_ball_robust(py,alphas):\n",
    "    '''modification to return selected index and handle the case where all values are 0'''\n",
    "    if len(py)!=len(alphas[0,:]):\n",
    "        raise ValueError('length of py (%i) and alpha (%i) does not match'%(len(py),len(alphas[0,:])))\n",
    "\n",
    "    if np.sum(alphas <= 0) > 0 or np.sum(alphas >= 1) > 0:\n",
    "        raise ValueError(['\"Alphas\" must be a real vector, with alpha range, the values must be within (0,1) interval'])\n",
    "\n",
    "    #Calculate dimension for each alpha\n",
    "    n = np.zeros((len(alphas[0,:])))\n",
    "    for i in range(len(alphas[0,:])):\n",
    "        if py[i] == 0:\n",
    "            #All points are separable. Nothing to do and not interesting\n",
    "            n[i]=np.nan\n",
    "        else:\n",
    "            p  = py[i]\n",
    "            a2 = alphas[0,i]**2\n",
    "            w = np.log(1-a2)\n",
    "            n[i] = lambertw(-(w/(2*np.pi*p*p*a2*(1-a2))))/(-w)\n",
    "\n",
    "    n[n==np.inf] = float('nan')\n",
    "    #Find indices of alphas which are not completely separable \n",
    "    inds = np.where(~np.isnan(n))[0]\n",
    "    if inds.size==0:\n",
    "        n_single_estimate = np.nan\n",
    "        alfa_single_estimate = np.nan\n",
    "        return n,n_single_estimate,alfa_single_estimate\n",
    "    else:\n",
    "        #Find the maximal value of such alpha\n",
    "        alpha_max = max(alphas[0,inds])\n",
    "        #The reference alpha is the closest to 90 of maximal partially separable alpha\n",
    "        alpha_ref = alpha_max*0.9\n",
    "        k = np.where(abs(alphas[0,inds]-alpha_ref)==min(abs(alphas[0,:]-alpha_ref)))[0]\n",
    "        #Get corresponding values\n",
    "        alfa_single_estimate = alphas[0,inds[k]]\n",
    "        n_single_estimate = n[inds[k]]\n",
    "\n",
    "        return n,n_single_estimate,alfa_single_estimate,inds[k]\n",
    "\n",
    "def point_inseparability_to_pointID(p_alpha,alphas=np.arange(.6,1,.02)[None], eps = 1e-100):\n",
    "    '''returns point ID from point inseparability values, fitting all alphas:\n",
    "    * instead of feeding mean point p_alphas and given alphas, we feed point p_alphas and given alphas to dimension_uniform_ball\n",
    "    * dimension_uniform_ball_robust is used instead of the base function to handle all 0 case and returns chosen alphas index \n",
    "    '''\n",
    "    \n",
    "    #correct for 0 to avoid NaNs\n",
    "    palphas_zero_corrected = p_alpha.T.copy()\n",
    "\n",
    "    n_singles = np.zeros(len(palphas_zero_corrected))\n",
    "    alpha_singles = np.zeros(len(palphas_zero_corrected))\n",
    "    alpha_idx = np.zeros(len(palphas_zero_corrected))\n",
    "\n",
    "    #for each point, check inseparability prob vector (one alpha-> one value)\n",
    "    for i,palpha in enumerate(palphas_zero_corrected):\n",
    "        n_alpha,n_singles[i],alpha_singles[i],alpha_idx[i] = dimension_uniform_ball_robust(palpha,alphas)\n",
    "    return n_singles, alpha_singles, alpha_idx\n",
    "\n",
    "from functools import wraps\n",
    "def calculate_time(func): \n",
    "    @wraps(func)\n",
    "    def inner_func(*args, **kwargs): \n",
    "        begin = time.time() \n",
    "        res = func(*args, **kwargs) \n",
    "        end = time.time()\n",
    "        return res, end - begin\n",
    "    return inner_func\n",
    "\n",
    "class DimEst():\n",
    "    def __init__(self):\n",
    "        self.names = ['MLE', 'GeoMLE', 'MIND', 'DANCo', 'FastDANCo', 'ESS', 'PCA', 'CD','FisherS','ANOVA','TwoNN']\n",
    "        self.caldatas = {}\n",
    "        \n",
    "    def estimateAllMethods(self, data):\n",
    "        dim = data.shape[1]\n",
    "        self.funcs = {'MLE':          self.mle(data),\n",
    "                      #'GeoMLE':       self.geomle(data, dim),\n",
    "                      'MiND':         self.mind_mlk(data, dim),\n",
    "                      #'DANCo':        self.danco(data, dim),\n",
    "                      'FastDANCo':    self.fast_danco(data),\n",
    "                      'ESS':          self.ess(data),\n",
    "                      #'PCA':          self.pca(data),\n",
    "                      #'CD':           self.cd(data),\n",
    "                      'FisherS':      self.fisherS(data),\n",
    "                      'ANOVA':        self.anova(data),\n",
    "                      'TwoNN':        self.twonn(data)\n",
    "                     }\n",
    "                      \n",
    "        self.times = {key: ig1(val) for key, val in self.funcs.items()}\n",
    "        self.funcs = {key: ig0(val) for key, val in self.funcs.items()}\n",
    "        return self.funcs, self.times\n",
    "    \n",
    "    def estimateAllMethodsLocally(self, data, k, n_jobs = 1):\n",
    "        dim = data.shape[1]\n",
    "        \n",
    "        _, knn = get_nn(data, k, n_jobs)\n",
    "        \n",
    "        self.funcs = {'MLE':          self.mlelocal(data,k),\n",
    "                      #'GeoMLE':       self.geomle(data, dim),\n",
    "                      'MiND':         asPointwise(data,self.mind_mlk,{'dim':dim},precomputed_knn=knn,n_jobs=1),\n",
    "                      #'DANCo':        asPointwise(data,self.danco,{'dim':dim},precomputed_knn=knn,n_jobs=1),\n",
    "                      'FastDANCo':    self.fast_dancoloop(data),\n",
    "                      'ESS':          asPointwise(data,self.ess,{},precomputed_knn=knn,n_jobs=1),\n",
    "                      #'PCA':          self.pca(data),\n",
    "                      #'CD':           self.cd(data),\n",
    "                      'FisherS':      asPointwise(data,self.fisherS,{},precomputed_knn=knn,n_jobs=n_jobs),\n",
    "                      'ANOVA':        self.anovalocal(data,k),\n",
    "                      'TwoNN':        asPointwise(data,self.twonn,{},precomputed_knn=knn,n_jobs=n_jobs)\n",
    "                     }\n",
    "                      \n",
    "        self.times = {}\n",
    "        for key, val in self.funcs.items():\n",
    "            if key in ['MLE','ANOVA','FastDANCo']:\n",
    "                self.funcs[key] = np.array(val[0])\n",
    "                self.times[key] = val[1]\n",
    "            else:\n",
    "                self.funcs[key] = np.array([i[0] for i in val])\n",
    "                self.times[key] = np.sum([i[1] for i in val])\n",
    "            \n",
    "        return self.funcs, self.times\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def mle(data):\n",
    "        return intdimr.maxLikGlobalDimEst(data,k=20).rx2('dim.est')[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def mlelocal(data,k):\n",
    "        res = intdimr.maxLikPointwiseDimEst(data,k=k)\n",
    "        return np.array([i[0] for i in res])\n",
    "\n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def geomle(data, dim):\n",
    "#         k1 =  k1_log(dim)\n",
    "#         k2 =  k2_log(dim)\n",
    "        return geomle(pd.DataFrame(data), k1=20, k2=55, nb_iter1=1, alpha=5e-3).mean()\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def mind_mlk(data, dim):\n",
    "        return intdimr.dancoDimEst(data, k=10, D=min(dim,100), ver=\"MIND_MLk\").rx2('dim.est')[0]\n",
    "    \n",
    "    #@staticmethod\n",
    "    @calculate_time\n",
    "    def danco(self,data, dim):\n",
    "        try:\n",
    "            res = intdimr.dancoDimEst(data, k=10, D=min(dim,100), calibration_data = self.caldatas[len(data)], ver=\"DANCo\")\n",
    "            self.caldatas[len(data)]=res[2]\n",
    "            return res.rx2('dim.est')[0]\n",
    "        except:\n",
    "            res = intdimr.dancoDimEst(data, k=10, D=min(dim,100), ver=\"DANCo\")\n",
    "            self.caldatas[len(data)]=res[2]\n",
    "            return res.rx2('dim.est')[0]\n",
    "\n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def fast_danco(data):\n",
    "        return runDANCo(data)[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def fast_dancoloop(data):\n",
    "        return runDANColoop(data)[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def ess(data):\n",
    "        return intdimr.essLocalDimEst(data).rx2('dim.est')[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def pca(data):\n",
    "        return intdimr.pcaLocalDimEst(data, 'FO').rx2('dim.est')[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def cd(data):\n",
    "        return corint_py(data, k1=10, k2=20)[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def fisherS(data):\n",
    "        return SeparabilityAnalysis(data,ProducePlots=0,alphas=np.arange(.2,1,.02)[None])[1][0]\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def anova(data):\n",
    "        return runANOVAglobal(data)[0,0]\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def anovalocal(data,k):\n",
    "        return runANOVAlocal(data,k=k)[:,0]\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def twonn(data):\n",
    "        res = TwoNN(data)\n",
    "        return res    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "DE=DimEst()\n",
    "#DG=DataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = list(filter(lambda x: '.data' in x, os.listdir('../data/id-tle-synth-m10000-data/data/m10000/')))\n",
    "synthetic_data = [np.array(pd.read_csv('../data/id-tle-synth-m10000-data/data/m10000/'+file,sep=' ',header=None)) for file in data_files]\n",
    "synthetic_data = dict(zip(data_files,synthetic_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global ID saturation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'synthetic_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-27a980992493>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# testing separability saturation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msynthetic_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mn_repeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msample_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'synthetic_data' is not defined"
     ]
    }
   ],
   "source": [
    "# testing separability saturation\n",
    "\n",
    "for key,data in synthetic_data.items():\n",
    "    n_repeats = 1\n",
    "    sample_sizes = [5,7,10,15,20,25,30,50,70,90]\n",
    "    sample_sizes_halves = [x/2 for x in sample_sizes]\n",
    "\n",
    "    all_sample_sizes = sample_sizes+sample_sizes_halves\n",
    "    all_sample_sizes = list(set(all_sample_sizes))\n",
    "    all_sample_sizes.sort()\n",
    "    \n",
    "    \n",
    "    datasets_done = [i.split('_')[0] for i in list(filter(lambda x: '.data' in x, os.listdir('../results')))]\n",
    "    dataset_name = key\n",
    "    n_samples = data.shape[0]\n",
    "    \n",
    "    if dataset_name in datasets_done:\n",
    "        print('already computed ', dataset_name)\n",
    "        continue\n",
    "    \n",
    "    print('\\n',dataset_name)\n",
    "    print('Running subsampling analysis...\\nSubsample percentages = {}\\nNumber of repeats = {}\\nNumber of samples = {}\\nDimension = {}'.format(sample_sizes,n_repeats,n_samples,data.shape[1]))\n",
    "    print('----------------------------\\n')\n",
    "\n",
    "    fisherS_dim_estimates = np.empty([len(all_sample_sizes)+1,n_repeats])\n",
    "    ess_dim_estimates = np.empty([len(all_sample_sizes)+1,n_repeats])\n",
    "    danco_dim_estimates = np.empty([len(all_sample_sizes)+1,n_repeats])\n",
    "    twonn_dim_estimates = np.empty([len(all_sample_sizes)+1,n_repeats])\n",
    "    anova_dim_estimates = np.empty([len(all_sample_sizes)+1,n_repeats])\n",
    "    mle_dim_estimates = np.empty([len(all_sample_sizes)+1,n_repeats])\n",
    "    mind_dim_estimates = np.empty([len(all_sample_sizes)+1,n_repeats])\n",
    "\n",
    "    runtimes = []\n",
    "    for i,sz in enumerate(all_sample_sizes):\n",
    "        sample_size = int(n_samples*sz/100)\n",
    "        print('Sample size = {}'.format(sample_size))\n",
    "        start_time = time.time()\n",
    "        for j in range(0,n_repeats):\n",
    "            sample = np.random.choice(n_samples,replace=False, size=sample_size)\n",
    "            xs = data[sample,:]\n",
    "\n",
    "            #Run estimators\n",
    "            allres = DE.estimateAllMethods(xs)\n",
    "            results = allres[0]\n",
    "            runtimes.append(allres[1])\n",
    "\n",
    "            #Store\n",
    "            fisherS_dim_estimates[i,j] = results['FisherS']\n",
    "            ess_dim_estimates[i,j] = results['ESS']\n",
    "            danco_dim_estimates[i,j] = results['FastDANCo']\n",
    "            twonn_dim_estimates[i,j] = results['TwoNN']\n",
    "            anova_dim_estimates[i,j] = results['ANOVA']\n",
    "            mle_dim_estimates[i,j] = results['MLE']\n",
    "            mind_dim_estimates[i,j] = results['MiND']\n",
    "\n",
    "\n",
    "        print(\"Elapsed time = {}\".format(time.time()-start_time))\n",
    "\n",
    "    allres = DE.estimateAllMethods(data)\n",
    "    results = allres[0]\n",
    "    runtimes.append(allres[1])\n",
    "\n",
    "    for i in range(0,n_repeats):\n",
    "        fisherS_dim_estimates[len(all_sample_sizes),i] = results['FisherS']\n",
    "        ess_dim_estimates[len(all_sample_sizes),i] = results['ESS']\n",
    "        danco_dim_estimates[len(all_sample_sizes),i] = results['FastDANCo']\n",
    "        twonn_dim_estimates[len(all_sample_sizes),i] = results['TwoNN']\n",
    "        anova_dim_estimates[len(all_sample_sizes),i] = results['ANOVA']\n",
    "        mle_dim_estimates[len(all_sample_sizes),i] = results['MLE']\n",
    "        mind_dim_estimates[len(all_sample_sizes),i] = results['MiND']\n",
    "\n",
    "\n",
    "    all_sample_sizes.append(100)\n",
    "    sample_sizes.append(100)\n",
    "\n",
    "    np.savetxt(\"../results/\"+dataset_name+\"_fisherS_dim_estimates.txt\", fisherS_dim_estimates, delimiter=\"\\t\")\n",
    "    np.savetxt(\"../results/\"+dataset_name+\"_ess_dim_estimates.txt\", ess_dim_estimates, delimiter=\"\\t\")\n",
    "    np.savetxt(\"../results/\"+dataset_name+\"_danco_dim_estimates.txt\", danco_dim_estimates, delimiter=\"\\t\")\n",
    "    np.savetxt(\"../results/\"+dataset_name+\"_twonn_dim_estimates.txt\", twonn_dim_estimates, delimiter=\"\\t\")\n",
    "    np.savetxt(\"../results/\"+dataset_name+\"_anova_dim_estimates.txt\", anova_dim_estimates, delimiter=\"\\t\")\n",
    "    np.savetxt(\"../results/\"+dataset_name+\"_mle_dim_estimates.txt\", mle_dim_estimates, delimiter=\"\\t\")\n",
    "    np.savetxt(\"../results/\"+dataset_name+\"_mind_dim_estimates.txt\", mind_dim_estimates, delimiter=\"\\t\")\n",
    "\n",
    "    np.savetxt(\"../results/\"+dataset_name+\"_all_sample_sizes.txt\", all_sample_sizes, delimiter=\"\\t\")\n",
    "    np.savetxt(\"../results/\"+dataset_name+\"_sample_sizes.txt\", sample_sizes, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot convergence curve\n",
    "alls=pd.read_csv('../results/'+dataset_name+'_all_sample_sizes.txt', sep='\\t',header=None)\n",
    "all_sample_sizes = alls.to_numpy()[:,0]\n",
    "sizes=pd.read_csv('../results/'+dataset_name+'_sample_sizes.txt', sep='\\t',header=None)\n",
    "sample_sizes =sizes.to_numpy()[:,0]\n",
    "\n",
    "estimators = ['fisherS_dim_estimates','ess_dim_estimates','danco_dim_estimates',\n",
    "              'twonn_dim_estimates','anova_dim_estimates','mle_dim_estimates','mind_dim_estimates']\n",
    "\n",
    "for estimator in estimators:\n",
    "    print(estimator)\n",
    "    ds=pd.read_csv('../results/'+dataset_name+'_'+estimator+'.txt', sep='\\t',header=None)\n",
    "    dim_estimates=ds.to_numpy()\n",
    "\n",
    "    mn = np.mean(dim_estimates[:,:],1)\n",
    "    std = np.std(dim_estimates[:,:],1)\n",
    "\n",
    "    plt.figure(figsize=(10,3))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(all_sample_sizes,mn,'bs-')\n",
    "    plt.plot(all_sample_sizes,mn-std,'r--')\n",
    "    plt.plot(all_sample_sizes,mn+std,'r--')\n",
    "    plt.plot(all_sample_sizes,dim_estimates,'b+')\n",
    "    plt.xlabel('Percentage of points')\n",
    "    plt.ylabel('Estimated intrinsic dimension')\n",
    "\n",
    "    ratios = []\n",
    "    for sz in sample_sizes:\n",
    "        sz_half = sz/2\n",
    "        k = [i for i,asz in enumerate(all_sample_sizes) if np.abs(sz-asz)<0.001 ][0]\n",
    "        k_half = [i for i,asz in enumerate(all_sample_sizes) if np.abs(sz_half-asz)<0.001 ][0]\n",
    "        #print(k,all_sample_sizes[k],k_half,all_sample_sizes[k_half])\n",
    "        ratios.append(1-std[k]/std[k_half])\n",
    "        \n",
    "    #avoid case of 0 std (nan ratio)\n",
    "    ratios=np.array(ratios)\n",
    "    ratios[np.isnan(ratios)]=1\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.plot(sample_sizes,ratios,'bs-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study kNN ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist\n",
      "Running kNN ID for all estimators...\n",
      "Number of samples = 2000\n",
      "Dimension = 784\n",
      "----------------------------\n",
      "\n",
      "kNN =  100\n"
     ]
    }
   ],
   "source": [
    "num_neighbors = [100]\n",
    "n_jobs = 4\n",
    "\n",
    "#Run ID estimators pointwise in KNN neighborhoods of different sizes\n",
    "for n_neighbors in num_neighbors:\n",
    "    for dataset_name,data in [('mnist',real_data_subsampled)]:\n",
    "    \n",
    "        n_samples = data.shape[0]\n",
    "        print(dataset_name)\n",
    "        print('Running kNN ID for all estimators...\\nNumber of samples = {}\\nDimension = {}'.format(n_samples,data.shape[1]))\n",
    "        print('----------------------------\\n')\n",
    "        print('kNN = ',n_neighbors)\n",
    "\n",
    "        start_all=time.time()\n",
    "\n",
    "        ests_pw_dict = DE.estimateAllMethodsLocally(data, k = n_neighbors, n_jobs = n_jobs)\n",
    "\n",
    "        print('elapsed :',round(time.time()-start_all,2))\n",
    "\n",
    "        with open('../results/ests_pw_dict_'+dataset_name+'_kNN'+str(n_neighbors)+'.pkl','wb') as f:\n",
    "            pickle.dump(ests_pw_dict,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../results/ests_pw_dict_'+dataset_name+'_kNN'+str(n_neighbors)+'.pkl','rb') as f:\n",
    "    res_pw=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res_pw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study global pointwise ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_global_id = []\n",
    "list_inseparability_id = []\n",
    "for dataset_name,data in list(real_data_subsampled.items())[:1]:\n",
    "\n",
    "    print(dataset_name)\n",
    "    start_all=time.time()\n",
    "\n",
    "    [n_alpha,n_single,p_alpha,alphas,separable_fraction,Xp] = SeparabilityAnalysis(data,ProducePlots=0)\n",
    "    palpha_selected = p_alpha[(n_alpha==n_single).tolist().index(True),:]\n",
    "    n_pointwise, alpha_singles, alpha_idx = point_inseparability_to_pointID(p_alpha)\n",
    "\n",
    "    list_global_id.append(n_single[0])\n",
    "    list_inseparability_id.append(n_pointwise)\n",
    "\n",
    "    print('elapsed :',round(time.time()-start_all,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study the behavior of the statistics used by the various estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
