{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geomle in /home/utilisateur/.local/lib/python3.7/site-packages (1.0)\n",
      "Requirement already satisfied: pandas>=0.19 in /home/utilisateur/.local/lib/python3.7/site-packages (from geomle) (0.25.3)\n",
      "Requirement already satisfied: numpy>=1.13.1 in /home/utilisateur/.local/lib/python3.7/site-packages (from geomle) (1.17.4)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /home/utilisateur/anaconda3/lib/python3.7/site-packages (from geomle) (0.21.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/utilisateur/anaconda3/lib/python3.7/site-packages (from pandas>=0.19->geomle) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/utilisateur/anaconda3/lib/python3.7/site-packages (from pandas>=0.19->geomle) (2.8.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /home/utilisateur/.local/lib/python3.7/site-packages (from scikit-learn>=0.18->geomle) (1.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/utilisateur/.local/lib/python3.7/site-packages (from scikit-learn>=0.18->geomle) (0.14.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/utilisateur/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas>=0.19->geomle) (1.13.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install geomle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from os import path\n",
    "current_folder = path.dirname(path.abspath('')) \n",
    "sys.path.append(current_folder)\n",
    "from estimators import *\n",
    "from geomle import geomle, mle, DataGenerator\n",
    "import multiprocessing as mp\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.linalg import cholesky\n",
    "from scipy.special import gammainc, lambertw\n",
    "import scipy.io\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import umap\n",
    "import seaborn as sns\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import rpy2\n",
    "import rpy2.robjects as ro\n",
    "import rpy2.robjects.numpy2ri\n",
    "import rpy2.robjects.packages as rpackages\n",
    "from functools import wraps\n",
    "import subprocess\n",
    "from IPython.display import display_html\n",
    "from operator import itemgetter\n",
    "ig0 = itemgetter(0)\n",
    "ig1 = itemgetter(1)\n",
    "ig2 = itemgetter(2)\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "utils = rpackages.importr('utils')\n",
    "#utils.install_packages('intrinsicDimension')\n",
    "#utils.install_packages('ider')\n",
    "intdimr = rpackages.importr('intrinsicDimension')\n",
    "ider   = rpackages.importr('ider')\n",
    "r_base = rpackages.importr('base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_side_by_side(*args):\n",
    "    html_str=''\n",
    "    for df in args:\n",
    "        html_str+=df.to_html()\n",
    "    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)\n",
    "\n",
    "def mean_sqe(estimations, truth):\n",
    "    '''\n",
    "    Mean squared error \n",
    "    '''\n",
    "    return ((estimations - truth)^2/truth).sum() /len(truth) \n",
    "    \n",
    "def mean_pe(estimations, truth):\n",
    "    '''\n",
    "    Mean percentage error \n",
    "    '''\n",
    "    return (abs(estimations - truth)/truth).sum() /len(truth)*100\n",
    "\n",
    "def mean_ge(estimations, truth):\n",
    "    '''\n",
    "    Mean geometric error: The geometric mean of the error *ratio*. It is always >= 1.\n",
    "    '''\n",
    "    ratios = np.concatenate(((estimations/truth)[np.newaxis, :], (truth/estimations)[np.newaxis, :]), axis=0)\n",
    "    return np.power(ratios.max(axis=0).prod(), 1.0/len(estimations))\n",
    "\n",
    "def med_pe(estimations, truth):\n",
    "    '''\n",
    "    Median error in %.\n",
    "    '''\n",
    "    return np.percentile(abs(estimations - truth)/truth, q=50)*100\n",
    "\n",
    "\n",
    "def randball(n_points,ndim,radius,center = []):\n",
    "    ''' Generate uniformly sampled ndim-sphere interior'''\n",
    "    if center == []:\n",
    "        center = np.array([0]*ndim)\n",
    "    r = radius\n",
    "    x = np.random.normal(size=(n_points, ndim))\n",
    "    ssq = np.sum(x**2,axis=1)\n",
    "    fr = r*gammainc(ndim/2,ssq/2)**(1/ndim)/np.sqrt(ssq)\n",
    "    frtiled = np.tile(fr.reshape(n_points,1),(1,ndim))\n",
    "    p = center + np.multiply(x,frtiled)\n",
    "    return p, center\n",
    "\n",
    "def proxy(tup):\n",
    "    function,X,Dict = tup\n",
    "    return function(X,**Dict)\n",
    "\n",
    "def get_nn(X,k,n_jobs=1):\n",
    "    neigh = NearestNeighbors(n_neighbors=k,n_jobs=n_jobs)\n",
    "    neigh.fit(X)\n",
    "    dists, inds = neigh.kneighbors(return_distance=True)\n",
    "    return dists,inds\n",
    "\n",
    "def asPointwise(data,function, params, precomputed_knn = None, n_neighbors=100, n_jobs=1):\n",
    "    '''Use a global estimator as a pointwise one by creating kNN neighborhoods'''\n",
    "    if precomputed_knn is not None:\n",
    "        knn = precomputed_knn\n",
    "    else:\n",
    "        _, knn = get_nn(data, k=n_neighbors, n_jobs=n_jobs)\n",
    "        \n",
    "    if n_jobs > 1:\n",
    "        pool = mp.Pool(n_jobs)\n",
    "        results = pool.map(proxy,[(function,data[i,:],params) for i in knn])\n",
    "        pool.close()\n",
    "        return results\n",
    "    else:\n",
    "        return [function(data[i,:],**params) for i in knn]\n",
    "\n",
    "\n",
    "from functools import wraps\n",
    "def calculate_time(func): \n",
    "    @wraps(func)\n",
    "    def inner_func(*args, **kwargs): \n",
    "        begin = time.time() \n",
    "        res = func(*args, **kwargs) \n",
    "        end = time.time()\n",
    "        return res, end - begin\n",
    "    return inner_func\n",
    "\n",
    "class DimEst():\n",
    "    def __init__(self):\n",
    "        self.names = ['MLE', 'GeoMLE', 'MIND', 'DANCo', 'FastDANCo', 'ESS', 'PCA', 'CD','FisherS','ANOVA','TwoNN']\n",
    "        self.caldatas = {}\n",
    "        \n",
    "    def estimateAllMethods(self, data,ConditionalNumber=10):\n",
    "        dim = data.shape[1]\n",
    "        self.funcs = {'MLE':          self.mle(data),\n",
    "                      #'GeoMLE':       self.geomle(data, dim),\n",
    "                      #'DANCo':        self.danco(data, dim),\n",
    "                      'FastDANCo':    self.fast_danco(data),\n",
    "                      #'ESS':          self.ess(data),\n",
    "                      'PCA':          self.pca(data),\n",
    "                      #'CD':           self.cd(data),\n",
    "                      'FisherS':      self.fisherS(data,ConditionalNumber),\n",
    "                      'ANOVA':        self.anova(data),\n",
    "                      'TwoNN':        self.twonn(data)\n",
    "                     }\n",
    "                      \n",
    "        self.times = {key: ig1(val) for key, val in self.funcs.items()}\n",
    "        self.funcs = {key: ig0(val) for key, val in self.funcs.items()}\n",
    "        return self.funcs, self.times\n",
    "    \n",
    "    def estimateAllMethodsLocally(self, data, k, n_jobs = 1, ConditionalNumber = 10):\n",
    "        dim = data.shape[1]\n",
    "        \n",
    "        _, knn = get_nn(data, k, n_jobs)\n",
    "        \n",
    "        mle_pw, tle_pw, mom_pw, ed_pw, ged_pw, pca_pw = self.rado_ests(data,k).values()\n",
    "        self.funcs = {'MLE':          self.mlelocal(data,k),\n",
    "                      #'GeoMLE':       self.geomlelocal(data, dim),\n",
    "                      'mind_mlk':         asPointwise(data,self.mind_mlk,{'dim':dim},precomputed_knn=knn,n_jobs=1),\n",
    "                      'mind_mli':         asPointwise(data,self.mind_mli,{'dim':dim},precomputed_knn=knn,n_jobs=1),\n",
    "                      #'DANCo':        asPointwise(data,self.danco,{'dim':dim},precomputed_knn=knn,n_jobs=1),\n",
    "                      'FastDANCo':    self.fast_dancoloop(data),\n",
    "                      'ESS':          asPointwise(data,self.ess,{},precomputed_knn=knn,n_jobs=n_jobs),\n",
    "                      #'PCA':          self.pca(data),\n",
    "                      'CD':           asPointwise(data,self.lcd,{},precomputed_knn=knn,n_jobs=n_jobs),\n",
    "                      'FisherS':      asPointwise(data,self.fisherS,{'ConditionalNumber':ConditionalNumber},precomputed_knn=knn,n_jobs=n_jobs),\n",
    "                      'ANOVA':        self.anovalocal(data,k),\n",
    "                      'TwoNN':        asPointwise(data,self.twonn,{},precomputed_knn=knn,n_jobs=n_jobs),\n",
    "                      'radoMLE':      mle_pw,\n",
    "                      'radoTLE':      tle_pw,\n",
    "                      'radoMOM':      mom_pw,\n",
    "                      'radoED':       ed_pw,\n",
    "                      'radoGED':      ged_pw,\n",
    "                      'radoPCA':      pca_pw\n",
    "                     }\n",
    "                      \n",
    "        self.times = {}\n",
    "        for key, val in self.funcs.items():\n",
    "            if key in ['MLE','ANOVA','FastDANCo']:\n",
    "                self.funcs[key] = np.array(val[0])\n",
    "                self.times[key] = val[1]\n",
    "            elif 'rado' in key:\n",
    "                self.funcs[key] = np.array(val)\n",
    "            else:\n",
    "                self.funcs[key] = np.array([i[0] for i in val])\n",
    "                self.times[key] = np.sum([i[1] for i in val])\n",
    "            \n",
    "        return self.funcs, self.times\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def rado_ests(data,k):\n",
    "        return radovanovic_estimators_matlab(data,k=k)\n",
    "        mle_pw, tle_pw, mom_pw, ed_pw, ged_pw, pca_pw = rado_ests.values()\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def mle(data):\n",
    "        return intdimr.maxLikGlobalDimEst(data,k=20).rx2('dim.est')[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def mlelocal(data,k):\n",
    "        res = intdimr.maxLikPointwiseDimEst(data,k=k)\n",
    "        return np.array([i[0] for i in res])\n",
    "\n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def geomle(data, dim):\n",
    "#         k1 =  k1_log(dim)\n",
    "#         k2 =  k2_log(dim)\n",
    "        return geomle(pd.DataFrame(data), k1=20, k2=55, nb_iter1=1, alpha=5e-3).mean()\n",
    "\n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def geomlelocal(data, dim):\n",
    "#         k1 =  k1_log(dim)\n",
    "#         k2 =  k2_log(dim)\n",
    "        return geomle(pd.DataFrame(data), k1=20, k2=55, nb_iter1=1, alpha=5e-3)\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def mind_mlk(data, dim):\n",
    "        return intdimr.dancoDimEst(data, k=10, D=min(dim,100), ver=\"MIND_MLk\").rx2('dim.est')[0]\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def mind_mli(data, dim):\n",
    "        return intdimr.dancoDimEst(data, k=10, D=min(dim,100), ver=\"MIND_MLi\").rx2('dim.est')[0]\n",
    "    \n",
    "    #@staticmethod\n",
    "    @calculate_time\n",
    "    def danco(self,data, dim):\n",
    "        try:\n",
    "            res = intdimr.dancoDimEst(data, k=10, D=min(dim,100), calibration_data = self.caldatas[len(data)], ver=\"DANCo\")\n",
    "            self.caldatas[len(data)]=res[2]\n",
    "            return res.rx2('dim.est')[0]\n",
    "        except:\n",
    "            res = intdimr.dancoDimEst(data, k=10, D=min(dim,100), ver=\"DANCo\")\n",
    "            self.caldatas[len(data)]=res[2]\n",
    "            return res.rx2('dim.est')[0]\n",
    "\n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def fast_danco(data):\n",
    "        return runDANCo(data)[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def fast_dancoloop(data):\n",
    "        return runDANColoop(data)\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def ess(data):\n",
    "        return ess_py(data)[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def pca(data):\n",
    "        return intdimr.pcaLocalDimEst(data, 'FO').rx2('dim.est')[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def cd(data):\n",
    "        return corint_py(data, k1=10, k2=20)[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def lcd(data):\n",
    "        return corint_py(data, k1=10, k2=len(data)-1)[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def fisherS(data,ConditionalNumber):\n",
    "        return SeparabilityAnalysis(data,ProducePlots=0,alphas=np.arange(.2,1,.02)[None],ConditionalNumber=ConditionalNumber)[1][0]\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def anova(data):\n",
    "        return runANOVAglobal(data)[0,0]\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def anovalocal(data,k):\n",
    "        return runANOVAlocal(data,k=k)[:,0]\n",
    "    \n",
    "    @staticmethod\n",
    "    @calculate_time\n",
    "    def twonn(data):\n",
    "        res = twonn_py(data)\n",
    "        return res    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DE=DimEst()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('m1-00.data', (10000, 11)),\n",
       " ('m10a-00.data', (10000, 11)),\n",
       " ('m10b-00.data', (10000, 18)),\n",
       " ('m10c-00.data', (10000, 25)),\n",
       " ('m11-00.data', (10000, 3)),\n",
       " ('m12-00.data', (10000, 20)),\n",
       " ('m13-00.data', (10000, 13)),\n",
       " ('m2-00.data', (10000, 5)),\n",
       " ('m3-00.data', (10000, 6)),\n",
       " ('m4-00.data', (10000, 8)),\n",
       " ('m5-00.data', (10000, 3)),\n",
       " ('m6-00.data', (10000, 36)),\n",
       " ('m7-00.data', (10000, 3)),\n",
       " ('m8-00.data', (10000, 72)),\n",
       " ('m9-00.data', (10000, 20))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_files = list(filter(lambda x: '.data' in x, os.listdir('../data/id-tle-synth-m10000-data/data/m10000/')))\n",
    "\n",
    "unique_datasets = np.unique([i.split('-')[0]+'-' for i in all_data_files])\n",
    "unique_datasets_00 = [file+'00.data' for file in unique_datasets]\n",
    "\n",
    "synthetic_data = [np.array(pd.read_csv('../data/id-tle-synth-m10000-data/data/m10000/'+file,sep=' ',header=None)) for file in unique_datasets_00]\n",
    "synthetic_data = dict(zip(unique_datasets_00,synthetic_data))\n",
    "\n",
    "list(zip(list(synthetic_data.keys()),[i.shape for i in synthetic_data.values()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global ID saturation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already computed  m1-00.data\n",
      "already computed  m10a-00.data\n",
      "already computed  m10b-00.data\n",
      "\n",
      " m10c-00.data\n",
      "Running subsampling analysis...\n",
      "Subsample percentages = [1, 2, 4, 8, 10, 20, 40, 60, 80]\n",
      "Number of repeats = 2\n",
      "Number of samples = 10000\n",
      "Dimension = 25\n",
      "----------------------------\n",
      "\n",
      "Random seed set to 0 before running the test\n",
      "Sample size = 100\n",
      "Elapsed time = 14.658308982849121\n",
      "Sample size = 200\n",
      "Elapsed time = 13.042473077774048\n",
      "Sample size = 400\n",
      "Elapsed time = 13.147257089614868\n",
      "Sample size = 800\n",
      "Elapsed time = 13.323329210281372\n",
      "Sample size = 1000\n",
      "Elapsed time = 13.56218934059143\n",
      "Sample size = 2000\n",
      "Elapsed time = 15.96231722831726\n",
      "Sample size = 4000\n",
      "Elapsed time = 26.25061559677124\n",
      "Sample size = 6000\n",
      "Elapsed time = 33.37805151939392\n",
      "Sample size = 8000\n",
      "Elapsed time = 54.82598066329956\n",
      "\n",
      " m11-00.data\n",
      "Running subsampling analysis...\n",
      "Subsample percentages = [1, 2, 4, 8, 10, 20, 40, 60, 80]\n",
      "Number of repeats = 2\n",
      "Number of samples = 10000\n",
      "Dimension = 3\n",
      "----------------------------\n",
      "\n",
      "Random seed set to 0 before running the test\n",
      "Sample size = 100\n",
      "Elapsed time = 13.078419923782349\n",
      "Sample size = 200\n",
      "Elapsed time = 12.917583465576172\n",
      "Sample size = 400\n",
      "Elapsed time = 13.699719429016113\n",
      "Sample size = 800\n",
      "Elapsed time = 13.165247678756714\n",
      "Sample size = 1000\n",
      "Elapsed time = 13.294655323028564\n",
      "Sample size = 2000\n",
      "Elapsed time = 14.061922311782837\n",
      "Sample size = 4000\n",
      "Elapsed time = 16.24298357963562\n",
      "Sample size = 6000\n",
      "Elapsed time = 20.537575721740723\n",
      "Sample size = 8000\n",
      "Elapsed time = 25.143789291381836\n",
      "\n",
      " m12-00.data\n",
      "Running subsampling analysis...\n",
      "Subsample percentages = [1, 2, 4, 8, 10, 20, 40, 60, 80]\n",
      "Number of repeats = 2\n",
      "Number of samples = 10000\n",
      "Dimension = 20\n",
      "----------------------------\n",
      "\n",
      "Random seed set to 0 before running the test\n",
      "Sample size = 100\n",
      "Elapsed time = 13.179567813873291\n",
      "Sample size = 200\n",
      "Elapsed time = 13.908397912979126\n",
      "Sample size = 400\n",
      "Elapsed time = 13.569442749023438\n",
      "Sample size = 800\n",
      "Elapsed time = 13.949048519134521\n",
      "Sample size = 1000\n",
      "Elapsed time = 13.542600631713867\n",
      "Sample size = 2000\n",
      "Elapsed time = 15.192398071289062\n",
      "Sample size = 4000\n",
      "Elapsed time = 23.72988724708557\n",
      "Sample size = 6000\n",
      "Elapsed time = 32.26020407676697\n",
      "Sample size = 8000\n",
      "Elapsed time = 47.75895667076111\n",
      "\n",
      " m13-00.data\n",
      "Running subsampling analysis...\n",
      "Subsample percentages = [1, 2, 4, 8, 10, 20, 40, 60, 80]\n",
      "Number of repeats = 2\n",
      "Number of samples = 10000\n",
      "Dimension = 13\n",
      "----------------------------\n",
      "\n",
      "Random seed set to 0 before running the test\n",
      "Sample size = 100\n",
      "Elapsed time = 13.064785480499268\n",
      "Sample size = 200\n",
      "Elapsed time = 13.151393413543701\n",
      "Sample size = 400\n",
      "Elapsed time = 12.646536111831665\n",
      "Sample size = 800\n",
      "Elapsed time = 13.148075103759766\n",
      "Sample size = 1000\n",
      "Elapsed time = 13.031761646270752\n",
      "Sample size = 2000\n",
      "Elapsed time = 14.566385746002197\n",
      "Sample size = 4000\n",
      "Elapsed time = 17.143954753875732\n",
      "Sample size = 6000\n",
      "Elapsed time = 20.846107721328735\n",
      "Sample size = 8000\n",
      "Elapsed time = 26.837269067764282\n",
      "\n",
      " m2-00.data\n",
      "Running subsampling analysis...\n",
      "Subsample percentages = [1, 2, 4, 8, 10, 20, 40, 60, 80]\n",
      "Number of repeats = 2\n",
      "Number of samples = 10000\n",
      "Dimension = 5\n",
      "----------------------------\n",
      "\n",
      "Random seed set to 0 before running the test\n",
      "Sample size = 100\n",
      "Elapsed time = 12.45590090751648\n",
      "Sample size = 200\n",
      "Elapsed time = 12.494216442108154\n",
      "Sample size = 400\n",
      "Elapsed time = 12.544326782226562\n",
      "Sample size = 800\n",
      "Elapsed time = 12.711400508880615\n",
      "Sample size = 1000\n",
      "Elapsed time = 12.843310594558716\n",
      "Sample size = 2000\n",
      "Elapsed time = 13.522550582885742\n",
      "Sample size = 4000\n",
      "Elapsed time = 16.117684602737427\n",
      "Sample size = 6000\n",
      "Elapsed time = 19.544343948364258\n",
      "Sample size = 8000\n",
      "Elapsed time = 25.40206241607666\n",
      "\n",
      " m3-00.data\n",
      "Running subsampling analysis...\n",
      "Subsample percentages = [1, 2, 4, 8, 10, 20, 40, 60, 80]\n",
      "Number of repeats = 2\n",
      "Number of samples = 10000\n",
      "Dimension = 6\n",
      "----------------------------\n",
      "\n",
      "Random seed set to 0 before running the test\n",
      "Sample size = 100\n",
      "Elapsed time = 13.7939453125\n",
      "Sample size = 200\n",
      "Elapsed time = 13.725662231445312\n",
      "Sample size = 400\n",
      "Elapsed time = 14.833096504211426\n",
      "Sample size = 800\n"
     ]
    }
   ],
   "source": [
    "# testing separability saturation\n",
    "n_repeats = 2\n",
    "all_sample_sizes = [1,2,4,8,10,20,40,60,80]\n",
    "\n",
    "for key,data in synthetic_data.items():\n",
    "    \n",
    "    datasets_done = [i.split('_')[0] for i in list(filter(lambda x: '.data' in x, os.listdir('../results')))]\n",
    "    dataset_name = key\n",
    "    n_samples = data.shape[0]\n",
    "    \n",
    "    if dataset_name in datasets_done:\n",
    "        print('already computed ', dataset_name)\n",
    "        continue\n",
    "    \n",
    "    print('\\n',dataset_name)\n",
    "    print('Running subsampling analysis...\\nSubsample percentages = {}\\nNumber of repeats = {}\\nNumber of samples = {}\\nDimension = {}'.format(all_sample_sizes,n_repeats,n_samples,data.shape[1]))\n",
    "    print('----------------------------\\n')\n",
    "    seed = 0; np.random.seed(seed)\n",
    "    print(f'Random seed set to {seed} before running the test')\n",
    "    \n",
    "    n_methods = 6\n",
    "    all_dim_estimates = np.empty([n_methods,len(all_sample_sizes)+1,n_repeats])\n",
    "\n",
    "    runtimes = []\n",
    "    for i,sz in enumerate(all_sample_sizes):\n",
    "        sample_size = int(n_samples*sz/100)\n",
    "        print('Sample size = {}'.format(sample_size))\n",
    "        start_time = time.time()\n",
    "        for j in range(0,n_repeats):\n",
    "            sample = np.random.choice(n_samples,replace=False, size=sample_size)\n",
    "            xs = data[sample,:]\n",
    "\n",
    "            #Run estimators\n",
    "            allres = DE.estimateAllMethods(xs)\n",
    "            results = allres[0]\n",
    "            runtimes.append(allres[1])\n",
    "\n",
    "            #Store\n",
    "            for it,key in enumerate(results.keys()):\n",
    "                all_dim_estimates[it,i,j] =  results[key]\n",
    "\n",
    "\n",
    "        print(\"Elapsed time = {}\".format(time.time()-start_time))\n",
    "\n",
    "    allres = DE.estimateAllMethods(data)\n",
    "    results = allres[0]\n",
    "    runtimes.append(allres[1])\n",
    "\n",
    "    for i in range(0,n_repeats):\n",
    "        for it,key in enumerate(results.keys()):\n",
    "            all_dim_estimates[it,len(all_sample_sizes),i] =  results[key]\n",
    "\n",
    "    with open(\"../results/\"+dataset_name+\"_all_dim_estimates.npy\",'wb') as f:\n",
    "        pickle.dump(all_dim_estimates,f)\n",
    "        \n",
    "all_sample_sizes.append(100)\n",
    "np.savetxt(\"../results/all_sample_sizes.txt\", all_sample_sizes, delimiter=\"\\t\")\n",
    "with open(\"../results/all_dim_estimates_keys.npy\",'wb') as f:\n",
    "        pickle.dump(list(results.keys()),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot convergence curve\n",
    "alls=pd.read_csv('../results/all_sample_sizes.txt', sep='\\t',header=None)\n",
    "all_sample_sizes = alls.to_numpy()[:,0]\n",
    "\n",
    "with open(\"../results/\"+dataset_name+\"_all_dim_estimates.npy\",'rb') as f:\n",
    "    all_dim_estimates = pickle.load(f)\n",
    "    \n",
    "with open(\"../results/all_dim_estimates_keys.npy\",'rb') as f:\n",
    "    estimators = pickle.load(f)\n",
    "\n",
    "for i,estimator in enumerate(estimators):\n",
    "    dim_estimates = all_dim_estimates[i]\n",
    "    \n",
    "    mn = np.mean(dim_estimates[:,:],1)\n",
    "    std = np.std(dim_estimates[:,:],1)\n",
    "\n",
    "    plt.figure(figsize=(10,3))\n",
    "    plt.subplot(121)\n",
    "    plt.title(estimator)\n",
    "    plt.plot(all_sample_sizes,mn,'bs-')\n",
    "    plt.plot(all_sample_sizes,mn-std,'r--')\n",
    "    plt.plot(all_sample_sizes,mn+std,'r--')\n",
    "    plt.plot(all_sample_sizes,dim_estimates,'b+')\n",
    "    plt.xlabel('Percentage of points')\n",
    "    plt.ylabel('Estimated intrinsic dimension')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local estimates convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing separability saturation\n",
    "n_jobs = 4\n",
    "num_neighbors = num_neighbors = np.arange(50,450,50)\n",
    "n_repeats = 2\n",
    "all_sample_sizes = [1,2,4,8,10,20,40,60,80]\n",
    "\n",
    "\n",
    "for n_neighbors in num_neighbors:\n",
    "\n",
    "    for key,data in synthetic_data.items():\n",
    "\n",
    "        datasets_done = [i.split('_')[0] for i in list(filter(lambda x: '.data' in x, os.listdir('../results')))]\n",
    "        dataset_name = key\n",
    "        n_samples = data.shape[0]\n",
    "\n",
    "        if dataset_name in datasets_done:\n",
    "            print('already computed ', dataset_name)\n",
    "            continue\n",
    "\n",
    "        print('\\n',dataset_name)\n",
    "        print('Running subsampling analysis...\\nSubsample percentages = {}\\nNumber of repeats = {}\\nNumber of samples = {}\\nDimension = {}'.format(all_sample_sizes,n_repeats,n_samples,data.shape[1]))\n",
    "        print('----------------------------\\n')\n",
    "        print('kNN = ',n_neighbors)\n",
    "        seed = 0; np.random.seed(seed)\n",
    "        print(f'Random seed set to {seed} before running the test')\n",
    "\n",
    "\n",
    "        n_methods = 16\n",
    "        all_dim_estimates = np.empty([n_methods,len(all_sample_sizes)+1,n_repeats,len(data)])\n",
    "\n",
    "        runtimes = []\n",
    "        for i,sz in enumerate(all_sample_sizes):\n",
    "            sample_size = int(n_samples*sz/100)\n",
    "            print('Sample size = {}'.format(sample_size))\n",
    "            start_time = time.time()\n",
    "            for j in range(0,n_repeats):\n",
    "                sample = np.random.choice(n_samples,replace=False, size=sample_size)\n",
    "                xs = data[sample,:]\n",
    "\n",
    "                #Run estimators\n",
    "                allres = DE.estimateAllMethodsLocally(xs, k = n_neighbors, n_jobs = n_jobs, ConditionalNumber = 10)\n",
    "\n",
    "                results = allres[0]\n",
    "                runtimes.append(allres[1])\n",
    "\n",
    "                #Store\n",
    "                for it,key in enumerate(results.keys()):\n",
    "                    all_dim_estimates[it,i,j,:len(xs)] =  results[key]\n",
    "\n",
    "\n",
    "            print(\"Elapsed time = {}\".format(time.time()-start_time))\n",
    "\n",
    "        allres = DE.estimateAllMethodsLocally(data, k = n_neighbors, n_jobs = n_jobs, ConditionalNumber = 10)\n",
    "\n",
    "        results = allres[0]\n",
    "        runtimes.append(allres[1])\n",
    "\n",
    "        for i in range(0,n_repeats):\n",
    "            for it,key in enumerate(results.keys()):\n",
    "                all_dim_estimates[it,len(all_sample_sizes),i,:len(data)] =  results[key]\n",
    "\n",
    "        with open(\"../results/\"+dataset_name+\"_all_dim_estimates_kNN\"+str(n_neighbors)+\".npy\",'wb') as f:\n",
    "            pickle.dump(all_dim_estimates,f)\n",
    "\n",
    "with open(\"../results/\"+dataset_name+\"_all_dim_estimates_keys_kNN\"++str(n_neighbors)+\".npy\",'wb') as f:\n",
    "        pickle.dump(list(results.keys()),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot convergence curve (to modify for local id)\n",
    "alls=pd.read_csv('../results/all_sample_sizes.txt', sep='\\t',header=None)\n",
    "all_sample_sizes = alls.to_numpy()[:,0]\n",
    "\n",
    "with open(\"../results/\"+dataset_name+\"_all_dim_estimates.npy\",'rb') as f:\n",
    "    all_dim_estimates = pickle.load(f)\n",
    "with open(\"../results/\"+dataset_name+\"_all_dim_estimates_keys_kNN\"++str(n_neighbors)+\".npy\",'wb') as f:\n",
    "    estimators = pickle.load(f)\n",
    "\n",
    "for i,estimator in enumerate(estimators):\n",
    "    dim_estimates = all_dim_estimates[i]\n",
    "    \n",
    "    mn = np.mean(dim_estimates[:,:],1)\n",
    "    std = np.std(dim_estimates[:,:],1)\n",
    "\n",
    "    plt.figure(figsize=(10,3))\n",
    "    plt.subplot(121)\n",
    "    plt.title(estimator)\n",
    "    plt.plot(all_sample_sizes,mn,'bs-')\n",
    "    plt.plot(all_sample_sizes,mn-std,'r--')\n",
    "    plt.plot(all_sample_sizes,mn+std,'r--')\n",
    "    plt.plot(all_sample_sizes,dim_estimates,'b+')\n",
    "    plt.xlabel('Percentage of points')\n",
    "    plt.ylabel('Estimated intrinsic dimension')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
